{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e1636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#import sklearn\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import apply_features\n",
    "import string\n",
    "import pickle\t# this is for saving and loading your trained classifiers.\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "##############################################################\n",
    "#\t\tIf you need, you may import other packages etc.      #\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59115832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename, data_type):\n",
    "\n",
    "    filepath = \"463_A1_TASK1_data/data/\"+ data_type+ \"/\" +filename \n",
    "    file = open(filepath, 'r')\n",
    "    lines = file.read().splitlines()\n",
    "    file.close()\n",
    "\n",
    "    processed = []\n",
    "    for line in lines:\n",
    "        \n",
    "        tokens = word_tokenize(line)\n",
    "\n",
    "       \n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        \n",
    "        \n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "        \n",
    "        preprocessed_line = ' '.join(tokens)\n",
    "        processed.append((preprocessed_line, filename.split(\"_\")[0]))  \n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f52656c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_megadoc():\n",
    "    training_documents = [\"philosophy_train.txt\",\"sports_train.txt\",\"mystery_train.txt\",\"religion_train.txt\",\"science_train.txt\",\"romance_train.txt\",\"horror_train.txt\",\"science-fiction_train.txt\"]\n",
    "    training_megadoc = []\n",
    "\n",
    "    for filename in training_documents:\n",
    "        training_megadoc.append(preprocess(filename,\"train\"))\n",
    "        #####\n",
    "#...\n",
    "    \n",
    "# Here, you may write the training_megadoc to a file. (You may also do it elsewhere or nowhere.)\n",
    "#...\n",
    "#####\n",
    "    return training_megadoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b691b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_megadoc():\n",
    "    test_documents = [\"philosophy_test.txt\",\"sports_test.txt\",\"mystery_test.txt\",\"religion_test.txt\",\"science_test.txt\",\"romance_test.txt\",\"horror_test.txt\",\"science-fiction_test.txt\"]\t\n",
    "    test_megadoc = []\n",
    "    \n",
    "    for filename in test_documents:\n",
    "        test_megadoc.append(preprocess(filename, \"test\"))\n",
    "        \n",
    "    \n",
    "    return test_megadoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14a3ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(megadoc):# megadoc can be either training_megadoc for training phase or test_megadoc for testing phase.\n",
    "    all_features = []\n",
    "    \n",
    "    for doc in megadoc:\n",
    "        \n",
    "        texts = [entry[0] for entry in doc]\n",
    "        label = doc[0][1]\n",
    "    \n",
    "        vectorizer = TfidfVectorizer(strip_accents='ascii', stop_words= 'english')\n",
    "        x = vectorizer.fit_transform(texts)\n",
    "    \n",
    "        features = vectorizer.get_feature_names_out()\n",
    "        all_features.append((features,label))\n",
    "        \n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1220421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier, training_set, model): #classifier is either nltk.NaiveBayesClassifier or SklearnClassifier(SVC()).\n",
    "    if(model == 'NB'):\n",
    "        nb_classifier = classifier.train(training_set)\n",
    "        return nb_classifier\n",
    "    \n",
    "    if(model == 'SVC'):\n",
    "        \n",
    "        svc_classifier = classifier.fit(training_set[0], training_set[1])\n",
    "        return svc_classifier\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a1a98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(classifier, test_set, classify_type):\n",
    "    if(classify_type == 'NB'):\n",
    "        nb = classifier.classify(test_set)\n",
    "        return nb\n",
    "    \n",
    "    if(classify_type == 'SVC'):\n",
    "        svc = classifier.predict(test_set)\n",
    "        return svc\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e29d70b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAIVE BAYES PREDICTIONS: ['philosophy', 'sports', 'mystery', 'religion', 'science', 'romance', 'horror', 'science-fiction']\n",
      "SVC PREDICTIONS []\n"
     ]
    }
   ],
   "source": [
    "def save_classifier(classifier, filename):\t#filename should end with .pickle and type(filename)=string\n",
    "\twith open(filename, \"wb\") as f:\n",
    "\t\tpickle.dump(classifier, f)\n",
    "\treturn\n",
    "\t\n",
    "\t\n",
    "def load_classifier(filename):\t#filename should end with .pickle and type(filename)=string\n",
    "\tclassifier_file = open(filename, \"rb\")\n",
    "\tclassifier = pickle.load(classifier_file)\n",
    "\tclassifier_file.close()\n",
    "\treturn classifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You may add or delete global variables.\n",
    "    training_set = []\n",
    "    test_set = []\n",
    "    dev_set = []\n",
    "    \n",
    "    \n",
    "    ##### TRAINING\n",
    "    training_set = create_training_megadoc()\n",
    "    \n",
    "    training = extract_features(training_set)\n",
    "    \n",
    "    \n",
    "    nb_going_train = []\n",
    "    svc_set = []\n",
    "    svc_labels = []\n",
    "    \n",
    "    for feat in training:\n",
    "        nb_feature_set = {}\n",
    "        \n",
    "        for i in feat[0]:\n",
    "            nb_feature_set[i] = True\n",
    "            svc_set.append(i)\n",
    "            svc_labels.append(feat[1])\n",
    "            \n",
    "        nb_going_train.append((nb_feature_set, feat[1]))\n",
    "    \n",
    "    nb_classifier = train(nltk.NaiveBayesClassifier, nb_going_train,'NB')\n",
    "    \n",
    "    svc_model = SVC()\n",
    "    #svc_classifier = train(svc_model, (svc_set, svc_labels),'SVC')\n",
    "    \n",
    "    #### DEVELOPMENT\n",
    "    #dev_set = create_test_megadoc()\n",
    "    \n",
    "    #developing = extract_features(dev_set)\n",
    "    #going_dev = []\n",
    "    \n",
    "    #for feat in developing:\n",
    "    #    feature_set = {}\n",
    "    #    for i in feat[0]:\n",
    "    #        feature_set[i] = True\n",
    "    #    predicted = test(nb_classifier,feature_set)\n",
    "    #    print(predicted)\n",
    "    #    going_dev.append(predicted)\n",
    "        \n",
    "    #print(going_dev)\n",
    "    \n",
    "    \n",
    "    ##### TESTING\n",
    "    test_set = create_test_megadoc()\n",
    "    \n",
    "    testing = extract_features(test_set)\n",
    "    nb_going_test = []\n",
    "    svc_going_test = []\n",
    "    \n",
    "    for feat in testing:\n",
    "        feature_set = {}\n",
    "        svc_test_set = []\n",
    "        \n",
    "        \n",
    "        for i in feat[0]:\n",
    "            feature_set[i] = True\n",
    "            svc_test_set.append(i)\n",
    "            #svc_test_labels.append(feat[1])\n",
    "            \n",
    "        nb_predicted = test(nb_classifier,feature_set,'NB')\n",
    "        nb_going_test.append(nb_predicted)\n",
    "        \n",
    "        #svc_predicted = test(svc_classifier, svc_test_set, 'SVC')\n",
    "        #svc_going_test.append(svc_predicted)\n",
    "        \n",
    "    print(\"NAIVE BAYES PREDICTIONS:\", nb_going_test)\n",
    "    print(\"SVC PREDICTIONS\", svc_going_test)  \n",
    "    ############## SVC APPROACH\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d050058",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
