{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f17c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbb2abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\uralp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\uralp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afbb2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text: tokenization, lowercasing, removing stopwords and punctuation.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d66dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_name):\n",
    "    \"\"\"\n",
    "    Read and parse an XML document, extracting sentences.\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r', encoding='iso-8859-5') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    sentences = re.findall(r'<sentence id=\"[^\"]*\">(.*?)</sentence>', content, re.DOTALL)\n",
    "   \n",
    "    # Strip leading and trailing whitespaces from each sentence\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    \n",
    "    elements_to_remove = ['\\n', '\\n1']\n",
    "    for element in elements_to_remove:\n",
    "        sentences = [sentence.replace(element, \"\") for sentence in sentences]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42595bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(folder_path):\n",
    "    \"\"\"\n",
    "    Get a list of XML file names in the given folder.\n",
    "    \"\"\"\n",
    "    return [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.xml')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b6efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(corpus):\n",
    "    \"\"\"\n",
    "    Calculate the TF-IDF matrix for the corpus.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    return vectorizer, vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95accfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document(file_name, vectorizer, tfidf_matrix, doc_index):\n",
    "    \"\"\"\n",
    "    Summarize the document specified by the file name.\n",
    "    \"\"\"\n",
    "    sentences = read_document(file_name)\n",
    "    preprocessed_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "    sentence_vectors = vectorizer.transform(preprocessed_sentences)\n",
    "\n",
    "    sim_scores = []\n",
    "    for i in range(len(sentences)):\n",
    "        print(i)\n",
    "        # Handle edge cases to avoid empty matrix\n",
    "        if len(sentences) == 1:\n",
    "            # If there's only one sentence, skip similarity calculation\n",
    "            continue\n",
    "\n",
    "        # Create a matrix excluding the current sentence\n",
    "        if i == 0:\n",
    "            matrix_without_sentence = tfidf_matrix[doc_index+1:]\n",
    "        elif i == len(sentences) - 1:\n",
    "            matrix_without_sentence = tfidf_matrix[:doc_index]\n",
    "        else:\n",
    "            matrix_without_sentence = vstack([tfidf_matrix[:doc_index], tfidf_matrix[doc_index+1:]])\n",
    "\n",
    "        # Check if matrix_without_sentence is not empty\n",
    "        if matrix_without_sentence.shape[0] > 0:\n",
    "            sim = cosine_similarity(sentence_vectors[i], matrix_without_sentence)\n",
    "            sim_scores.append(np.mean(sim))\n",
    "\n",
    "    # Select top 5 sentences if there are enough sentences\n",
    "    if len(sim_scores) > 0:\n",
    "        top_indices = np.argsort(sim_scores)[-5:]\n",
    "        summary = [sentences[i] for i in top_indices]\n",
    "    else:\n",
    "        summary = []\n",
    "\n",
    "    for sentence in summary:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "800b8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder containing the XML documents\n",
    "folder_path = 'HW2_Data/Task2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82811d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all documents and create a corpus\n",
    "file_names = get_file_names(folder_path)\n",
    "corpus = []\n",
    "for file in file_names:\n",
    "    corpus.extend(read_document(file))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3880f43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 54674)\t0.15903892594983834\n",
      "  (0, 3791)\t0.07811157963427807\n",
      "  (0, 54644)\t0.08570722989696063\n",
      "  (0, 4368)\t0.08915683076230238\n",
      "  (0, 61613)\t0.042084437580329294\n",
      "  (0, 89189)\t0.13037258052494585\n",
      "  (0, 22338)\t0.041779721981865456\n",
      "  (0, 53915)\t0.060526493705818064\n",
      "  (0, 61930)\t0.16146726899338937\n",
      "  (0, 66644)\t0.14312014186419272\n",
      "  (0, 16924)\t0.11962978508629077\n",
      "  (0, 61363)\t0.02400477218482631\n",
      "  (0, 79034)\t0.08121248752540744\n",
      "  (0, 33572)\t0.07672452506422192\n",
      "  (0, 16345)\t0.05814396365605151\n",
      "  (0, 43246)\t0.34298147705306514\n",
      "  (0, 15572)\t0.16043306337301327\n",
      "  (0, 60022)\t0.18659101992988994\n",
      "  (0, 58201)\t0.15257378053427398\n",
      "  (0, 15375)\t0.03120596410136165\n",
      "  (0, 43944)\t0.1117132062440983\n",
      "  (0, 52270)\t0.2956725987743083\n",
      "  (0, 53294)\t0.15258974613154305\n",
      "  (0, 59689)\t0.29347811512745137\n",
      "  (0, 75257)\t0.633026969300207\n",
      "  :\t:\n",
      "  (835766, 65735)\t0.12082088593336131\n",
      "  (835766, 17482)\t0.13530905987348824\n",
      "  (835766, 16970)\t0.0662835373930054\n",
      "  (835766, 25643)\t0.0659597919750884\n",
      "  (835766, 29488)\t0.05602781182898695\n",
      "  (835766, 4592)\t0.11798764466141927\n",
      "  (835766, 43012)\t0.050058317030419845\n",
      "  (835766, 76912)\t0.2434400043445938\n",
      "  (835766, 15216)\t0.11516161128682452\n",
      "  (835766, 27937)\t0.21744378682409815\n",
      "  (835766, 23540)\t0.05751897822459354\n",
      "  (835766, 52434)\t0.057398457297943246\n",
      "  (835766, 29486)\t0.10607097148313435\n",
      "  (835766, 28704)\t0.0544068563957883\n",
      "  (835766, 58173)\t0.1306963837016974\n",
      "  (835766, 37087)\t0.05469776979211261\n",
      "  (835766, 74850)\t0.057405691521201\n",
      "  (835766, 48775)\t0.05043698491974146\n",
      "  (835766, 38352)\t0.21047433904617927\n",
      "  (835766, 16067)\t0.07690501043150373\n",
      "  (835766, 27607)\t0.06231235484760602\n",
      "  (835766, 54644)\t0.11280428197287617\n",
      "  (835766, 61363)\t0.03159408014338156\n",
      "  (835766, 15375)\t0.04107198865203651\n",
      "  (835766, 82012)\t0.05061665052588637\n"
     ]
    }
   ],
   "source": [
    "# Calculate TF-IDF for the entire corpus\n",
    "vectorizer, tfidf_matrix = calculate_tfidf(corpus)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "366978a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "I also accept that this may become a matter about which questions might be asked if the orders sought by the applicants are made.\n",
      "7 At least for those reasons, this Court has held on a number of occasions that typically a party seeking leave to appeal from an interlocutory judgment ought to establish, first, that in all the circumstances, the decision from which leave is sought to appeal is attended with sufficient doubt to warrant the same being reconsidered by the Full Court, and secondly, that substantial injustice would result if such leave was to be refused, supposing the decision to have been wrong: see D&eacute;cor at 398.\n",
      "Lord Justice Slade made those remarks in the course of overruling the decision of Scott J at first instance, which decision had been to the effect that a court did not have the power under s 37 of the Supreme Court Act 1981 (UK) to make an order requiring cross-examination ancillary to compliance with a Mareva order, in the absence of circumstances whereby a justiciable issue is before the Court in respect of which the evidence of the deponent is relevant to the resolution thereof, for instance where the deponent of the disclosure affidavit was the subject of a contempt motion.\n",
      "The Sharman applicants relied upon a further passage from the majority's reasons for judgment in Gerlach at [13], as follows:  'The principles governing the grant of leave to appeal against interlocutory orders are well established...If it is plain that wrong principle was applied by the judge considering the application, it may well be that leave should be granted...'  13 The Music companies, for their part, placed particular reliance upon the joint reasons for judgment of Black CJ and Stone J in Brilliant Digital for the approach that they contended ought to be adopted in my consideration of the present application for leave to appeal.\n",
      "Each of the Sharman applicants was one of ten respondents to infringement of copyright proceedings brought by the present respondents ('the Music companies') in respect of the operation of what was described by the parties as the 'Kazaa system' ('the primary proceedings').\n"
     ]
    }
   ],
   "source": [
    "# Summarize a specific document\n",
    "document_to_summarize = \"HW2_Data/Task2/06_1.xml\"\n",
    "doc_index = file_names.index(document_to_summarize)\n",
    "summarize_document(document_to_summarize, vectorizer, tfidf_matrix, doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7331854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab35d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
